"""
Generate the metadata.csv & waverforms.hdf5 files from PICTS data. Constants
(variables IN_CAPS at the top of the file) specify the file locations.

Notes:

The log file only contains certain logs that I am able to capture;
most warnings from seisbench go to stdout and I haven't found a neat solution
to log them yet.
The methods of picking the wave arrivals for the metadata and the train / dev /
test split could probably be improved.

Credit to the following where some code was taken form or inspired by:
"dataset_test040823.ipynb" from Szymon &
https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/03b_creating_a_dataset.ipynb
"""


import obspy
from obspy import UTCDateTime
from obspy import Stream
from obspy.taup import TauPyModel
from obspy.taup import taup_geo

import pandas as pd
import os
import sys
import numpy as np
from datetime import datetime

import pyrocko
from pyrocko import obspy_compat

import seisbench.data as sbd
import seisbench.util as sbu


# setup compatability between obspy and pyrocko
obspy_compat.plant()


# file containing a catalogue of events
EVENTS_CATALOGUE = "pre-processed_event_data/preprocessed_event_data_2023-08-22 08:07:33.029061.csv"
# base paths for the PICTS data
PICTS_DATA_PATHS = [
    "/run/media/dv1/Dave_T5/PICTS/PICTS_data/",
    "/run/media/dv1/Dave_T5/PICTS/PICTS_pullout/",
]
# file containing lon-lat of PICTS stations
STATION_INFO = "other_data_files/picts_network_info.csv"
# output file paths
METADATA_PATH = "generated_training_data/metadata.csv"
WAVEFORMS_PATH = "generated_training_data/waveforms.hdf5"
LOGFILE_PATH = "generated_training_data/generator.log"
# constants for use with TauPyModel
EARTH_RADIUS = (
    6363.133  # taken from https://rechneronline.de/earth-radius/ for lat = 57
)
EARTH_FLATTENING = 0


def main():
    check_dir_ready()
    log_file("generation started")
    # create the random number generator used for test / train / validate split
    r_num_gen = np.random.default_rng(53485178)
    # load info (lon-lat etc) for stations
    station_info = pd.read_csv(STATION_INFO, header=[0])
    # load events catalogue
    events = pd.read_csv(EVENTS_CATALOGUE, header=[0])
    # model of estimation for p/s arrival times
    taupy_model = TauPyModel(model="ak135")

    # write the files
    with sbd.WaveformDataWriter(METADATA_PATH, WAVEFORMS_PATH) as writer:
        writer.data_format = {"dimension_order": "CW", "component_order": "ZNE"}

        # iterate over the events
        for event_index in events.index:
            # iterate over the stations
            for station_index in station_info.index:
                station_name = station_info.loc[station_index, "name"],
                p_column = station_name[0] + "_p_arrival_time_man_picked"
                s_column = station_name[0] + "_s_arrival_time_man_picked"
                try:
                    p_arrival_time = UTCDateTime(events.loc[event_index, p_column])
                    s_arrival_time = UTCDateTime(events.loc[event_index, s_column])
                except TypeError:
                    # manual pick hasnt been made
                    continue
                # NOTE that waves occuring over midnight have not been accounted for
                streams = Stream()
                # if all waves arrival estimates are on the same day
                if p_arrival_time.julday == s_arrival_time.julday:
                    file_paths = get_files(
                        network=station_info.loc[station_index, "network"],
                        station=station_info.loc[station_index, "name"],
                        year=str(p_arrival_time.year),
                        jday=str(p_arrival_time.julday),
                    )
                    streams += get_streams(
                        file_paths=file_paths,
                        start=(p_arrival_time - 30),
                        end=(p_arrival_time + 30),
                    )
                # if wave arrival estimates are on different days
                else:
                    file_paths = get_files(
                        network=station_info.loc[station_index, "network"],
                        station=station_index,
                        year=str(p_arrival_time.year),
                        jday=str(p_arrival_time.julday),
                    )
                    streams += get_streams(
                        file_paths=file_paths,
                        start=(p_arrival_time - 30),
                        end=(p_arrival_time + 30),
                    )
                    file_paths = get_files(
                        network=station_info.loc[station_index, "network"],
                        station=station_index,
                        year=str(s_arrival_time.year),
                        jday=str(s_arrival_time.julday),
                    )
                    streams += get_streams(
                        file_paths=file_paths,
                        start=(s_arrival_time - 30),
                        end=(p_arrival_time + 30),
                    )
                if not streams:
                    log_file(f"No streams found for station: {station_name}, event time: {events.loc[event_index, 'utcdate']}")
                    continue
                # check that all streams have the same sampling rate
                for trace in streams:
                    if trace.stats.sampling_rate != streams[0].stats.sampling_rate:
                        log_file(f"Streams had different sampling rates (so not written) for station: {station_info.loc[station_index, 'name']}, event time: {events.loc[event_index, 'utcdate']}")
                        continue
                # get streams formatted for writing
                actual_t_start, data, _ = sbu.stream_to_array(
                    streams, component_order=writer.data_format["component_order"]
                )
                # put metadata into a dictionary ready for writing
                metadata = compose_metadata(
                    event=events.loc[event_index],
                    station_info=station_info.loc[station_index],
                    sampling_rate=streams[0].stats.sampling_rate,
                    p_travel_time=None,
                    start=actual_t_start,
                    rng=r_num_gen,
                    p_arrival=p_arrival_time,
                    s_arrival=s_arrival_time,
                )
                # add data to the writer
                writer.add_trace(metadata, data)
    log_file("Finished")


def log_file(message: str) -> None:
    """Adds timestap & message to the logfile."""
    with open(LOGFILE_PATH, "a") as file:
        log_message = f"{datetime.now()}:    {message}\n"
        file.write(log_message)
    print(log_message)
    return


def check_dir_ready() -> None:
    """Checks that generator_outputs directory is empty."""
    # check the directory exists
    if not os.path.exists("generator_outputs"):
        os.mkdir("generator_outputs")
        if not os.path.exists("generator_outputs"):
            raise FileNotFoundError(
                "Something went wrong creating dir: generator_outputs"
            )
    # check directory is empty
    if os.listdir("generator_outputs"):
        sys.exit(
            "Check failed: The generator_outputs directory already has files in it"
        )
    return


def compose_metadata(
    *,
    event,
    station_info,
    sampling_rate,
    p_travel_time,
    start,
    rng,
    p_arrival,
    s_arrival,
) -> dict[str, str | None | float | int]:
    rand_num = int(rng.integers(low=0, high=10, size=1))
    # TODO consider more optimal way to split data
    if rand_num <= 1:
        chosen_split = "test"
    elif rand_num <= 3:
        chosen_split = "dev"
    else:
        chosen_split = "train"
    p_sample_arrival = (p_arrival - start) * sampling_rate
    s_sample_arrival = (s_arrival - start) * sampling_rate
    return {
        "station_network_code": station_info["network"],
        "station_code": station_info.name,
        "trace_channel": "HH",
        "station_latitude_deg": station_info["lat"],
        "station_longitude_deg": station_info["lon"],
        "station_elevation_m": 0,  # NOTE this is populated in the STEAD dataset
        "trace_p_arrival_sample": p_sample_arrival,
        "trace_p_status": None,  # NOTE this is populated in the STEAD dataset
        "trace_p_weight": None,  # NOTE this is populated in the STEAD dataset
        "path_p_travel_sec": p_travel_time,
        "trace_s_arrival_sample": s_sample_arrival,
        "trace_s_status": None,  # NOTE this is populated in the STEAD dataset
        "trace_s_weight": None,  # NOTE this is populated in the STEAD dataset
        "source_id": None,  # NOTE this is populated in the STEAD dataset
        "source_origin_time": event["utc_datetime"],
        "source_origin_uncertainty_sec": None,  # NOTE this is populated in the STEAD dataset
        "source_latitude_deg": event["lat"],
        "source_longitude_deg": event["lon"],
        "source_error_sec": None,  # NOTE this is populated in the STEAD dataset
        "source_gap_deg": None,  # NOTE this is populated in the STEAD dataset
        "source_horizontal_uncertainty_km": None,  # NOTE this is populated in the STEAD dataset
        "source_depth_km": event["depth"],
        "source_depth_uncertainty_km": None,  # NOTE this is populated in the STEAD dataset
        "source_magnitude": event["ML"],
        "source_magnitude_type": "ml",
        "source_magnitude_author": None,  # NOTE this is populated in the STEAD dataset
        "source_mechanism_strike_dip_rake": None,  # NOTE this is populated in the STEAD dataset
        "source_distance_deg": None,  # NOTE this is populated in the STEAD dataset
        "source_distance_km": None,  # NOTE this is populated in the STEAD dataset
        "path_back_azimuth_deg": None,  # NOTE this is populated in the STEAD dataset
        "trace_snr_db": None,  # NOTE this is populated in the STEAD dataset
        "trace_coda_end_sample": None,  # NOTE this is populated in the STEAD dataset
        "trace_start_time": start,
        "trace_category": None,  # NOTE this is populated in the STEAD dataset
        "trace_name": None,  # NOTE this is populated in the STEAD dataset
        "split": chosen_split,
        "trace_name_original": None,  # NOTE this is populated in the STEAD dataset
        "trace_sampling_rate_hz": sampling_rate,
    }


def get_streams(
    *, file_paths: list[str], start: UTCDateTime, end: UTCDateTime
) -> Stream | None:
    """Returns all streams that match the arguments provided"""
    streams = Stream()
    for path in file_paths:
        streams += obspy.read(path, starttime=start, endtime=end)
    return streams


def get_files(
    *, network: str, station: str, year: str | int, jday: str | int
) -> list[str | None]:
    """Returns a list of file paths for files that match the arguments"""
    year, jday = str(year), str(jday)
    file_list: list[str] = list()
    for path in PICTS_DATA_PATHS:
        location = os.path.join(path, year, network, station)
        for root, dirs, files in os.walk(location):
            for file in files:
                if not file.startswith(".") and file.endswith(jday):
                    file_list.append(os.path.join(root, file))
    return file_list


if __name__ == "__main__":
    main()
